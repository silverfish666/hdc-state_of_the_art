# Hyperdimensional Computing: State of the art
The existing world of Artificial Intelligence (AI) is around experts (usually with master degree), expensive hardware, and big data. That makes AI not affordable to most companies, needless to mention individuals. There are undergoing many improvements to the existing AI and machine learning (e.g. deep learning). However, the bar is still high. We depend heavily right now on expensive processing units like GPU with high energy consumption. We need a new paradigm the shift the AI industry focus from relying on expensive computation and big data to much simpler approach. This approach should have lower barrier of entry like learning, hardware cost and ongoing machine learning as all data is not available at the beginning. But something we can learn from the deep learning revolution, that hardware accelerators can boost the adoption of AI solutions.

Hyperdimensional Computing (HDC) can satisfy most of the conditions above with performance that can match more mature solutions e.g. Deep Learning. However, there are some challenges facing the wide adoption of HDC. The existing hardware market is in more favor of traditional von Neumann architecture. To achieve the highest value of HDC, we need to build a new hardware from scratch. This is could be high cost and delays for time to market. So, the best for HDC right now is try to make the most of existing state of the art technologies to approximate the optimal performance of HDC.

